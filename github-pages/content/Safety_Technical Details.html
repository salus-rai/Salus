<!DOCTYPE html>
<html><head></head><body><div class="main-panel wiki-content">
<div class="description">
<div class="header-wrapper" style="max-width: initial">
<!-- breadcrumbs could be implemented and inserted here -->
<div></div>

</div>
<div class="header-wrapper">
<div class="title-wrapper">
<div class="title">
<h1 style="display: flex; align-items: center;">
                                        Safety - Comprehensive Guide of Safety
                                    </h1>
</div>
<div class="row" style="display: flex;padding: 0; ">

</div>
</div>
</div>
<div class="content-wrapper">
<style>@media (prefers-color-scheme: dark) { }</style>
<div class="toc-macro client-side-toc-macro conf-macro output-block" data-cssliststyle="none" data-hasbody="false" data-headerelements="H1,H2,H3,H4,H5,H6" data-layout="default" data-local-id="a44a8f6d-d4a0-462f-b138-89bedd915e7d" data-macro-id="10441671-dde5-4153-97e7-c70e90e9861d" data-macro-name="toc" data-numberedoutline="false" data-structure="list">
<style>[data-colorid=hxv9kpniy0]{color:#0747a6} html[data-color-mode=dark] [data-colorid=hxv9kpniy0]{color:#5999f8}[data-colorid=a5t3qo25k2]{color:#0747a6} html[data-color-mode=dark] [data-colorid=a5t3qo25k2]{color:#5999f8}[data-colorid=gkzb5ju5po]{color:#0747a6} html[data-color-mode=dark] [data-colorid=gkzb5ju5po]{color:#5999f8}[data-colorid=k6rauc56g7]{color:#0747a6} html[data-color-mode=dark] [data-colorid=k6rauc56g7]{color:#5999f8}[data-colorid=kjebry1frf]{color:#0747a6} html[data-color-mode=dark] [data-colorid=kjebry1frf]{color:#5999f8}[data-colorid=vvodtufps4]{color:#0747a6} html[data-color-mode=dark] [data-colorid=vvodtufps4]{color:#5999f8}</style>
<ul>
<li><a class="not-blank" href="#DetailedLookAtOurPrivacyModules"><span data-colorid="vvodtufps4">Detailed Look At Our Privacy Modules</span></a>
<ul>
<li><a class="not-blank" href="#ListofMethodsforUnstructuredTextData"><span data-colorid="gkzb5ju5po">List of Methods for Unstructured Text Data</span></a>
<ul>
<li><a class="not-blank" href="#SafetyAnalyze">Safety Analyze</a></li>
<li><a class="not-blank" href="#SafetyAnonymize">Safety Anonymize</a></li>
</ul></li>
<li><a class="not-blank" href="#ListofMethodsforImageData"><span data-colorid="hxv9kpniy0">List of Methods for Image Data</span></a>
<ul>
<li><a class="not-blank" href="#ImageAnalyze">Image Analyze</a></li>
<li><a class="not-blank" href="#ImageGenerate">Image Generate</a></li>
<li><a class="not-blank" href="#NudityAnalyze">Nudity Analyze</a></li>
</ul></li>
<li><a class="not-blank" href="#ListofmethodsforVideoData"><span data-colorid="kjebry1frf">List of methods for Video Data</span></a>
<ul>
<li><a class="not-blank" href="#SafetyVideo">Safety Video</a></li>
<li><a class="not-blank" href="#NudityVideo">Nudity Video</a></li>
</ul></li>
</ul></li>
<li><a class="not-blank" href="#ModelsUsed"><span data-colorid="k6rauc56g7">Models Used</span></a>
<ul>
<li><a class="not-blank" href="#DetoxifyModel"><u>Detoxify Model</u></a></li>
<li><a class="not-blank" href="#NSFWGantman"><u>NSFW Gantman</u></a></li>
<li><a class="not-blank" href="#NudeNetModel"><u>NudeNet Model</u></a></li>
</ul></li>
<li><a class="not-blank" href="#SafetyByExamples"><span data-colorid="a5t3qo25k2">Safety - By Examples</span></a>
<ul>
<li><a class="not-blank" href="#UnstructuredTextSafety"><u>Unstructured Text Safety</u></a></li>
<li><a class="not-blank" href="#ImageSafety"><u>Image Safety</u></a></li>
</ul></li>
</ul>
</div>
<h2 id="DetailedLookAtOurPrivacyModules"><span data-colorid="vvodtufps4">Detailed Look At Our Privacy Modules</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="0f8a0d4f-30d5-4741-89c8-8a84654249fc" data-macro-name="expand" id="expander-497236873">
<div class="expand-control" id="expander-control-497236873" onclick="expandContent('expander-content-497236873', 'expander-control-497236873')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Unstructured Text</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-497236873">
<h3 id="ListofMethodsforUnstructuredTextData"><span data-colorid="gkzb5ju5po">List of Methods for Unstructured Text Data</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<div class="table-wrap dt626732093" style="width: 100%; margin: 10px auto 0px; max-width: 760px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="ac5c6854-043c-41a7-b77e-dc9468838661" style="border-collapse: collapse; width: 100%;">
<tbody>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Method</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Description</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Request</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Response</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><h6 id="SafetyAnalyze">Safety Analyze<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h6></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Detects profane words in each text and gives JSON report as an output.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Mandatory Fields: Input text</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>JSON Report</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><h6 id="SafetyAnonymize">Safety Anonymize<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h6></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Anonymizing detected profane words in each text and gives anonymized text as an output.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Mandatory Fields: Input</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Anonymized Text</p></td>
</tr>
</tbody>
</table></div>
<style>html .dt626732093 table {min-width:0px;}</style>
</div>
</div>
</div>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="c7509282-df2c-4de3-927e-687f225525c5" data-macro-name="expand" id="expander-1191184815">
<div class="expand-control" id="expander-control-1191184815" onclick="expandContent('expander-content-1191184815', 'expander-control-1191184815')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Image</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1191184815">
<h3 id="ListofMethodsforImageData"><span data-colorid="hxv9kpniy0">List of Methods for Image Data</span> <a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<div class="table-wrap dt-1284670463" style="width: 100%; margin: 10px auto 0px; max-width: 760px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="4be37fbb-d44e-4d2a-b9da-31178a504019" style="border-collapse: collapse; width: 100%;">
<tbody>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Method</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Description</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Request</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Response</p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><h6 id="ImageAnalyze">Image Analyze<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h6></td>
<td class="confluenceTd" rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Detects NSFW image based on parameters (porn, sexy, neutral, drawings, hentai) in input image.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Mandatory Fields: Input Image</p></td>
<td class="confluenceTd" rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>JSON containing the detection scores and byte64 code of output image, all in JSON format</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Optional Fields:  portfolio, account</p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><h6 id="ImageGenerate">Image Generate<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h6></td>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Generates an image based on the prompt given as a input.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Mandatory Fields: Input Prompt</p></td>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Gives output image based on the prompt and also checks the image using above image analyze api. Gives JSON containing the detection scores and byte64 code of output image, all in JSON format</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Optional Fields:  portfolio, account</p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><h6 id="NudityAnalyze">Nudity Analyze<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h6></td>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Detects the specific parts of nudity in the given image.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Mandatory Fields: Input Image</p></td>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Gives byte64 image blurring the detected nudity parts.</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Optional Fields:  portfolio, account</p></td>
</tr>
</tbody>
</table></div>
<style>html .dt-1284670463 table {min-width:0px;}</style>
</div>
</div>
</div>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="3ba19031-0634-4c5c-abe6-41241bb0c4fa" data-macro-name="expand" id="expander-1798279160">
<div class="expand-control" id="expander-control-1798279160" onclick="expandContent('expander-content-1798279160', 'expander-control-1798279160')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Video</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1798279160">
<h3 id="ListofmethodsforVideoData"><span data-colorid="kjebry1frf">List of methods for Video Data</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<div class="table-wrap dt-777637637" style="width: 100%; margin: 10px auto 0px; max-width: 760px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="12d427da-7108-4ff9-9683-ec9ef40f300b" style="border-collapse: collapse; width: 100%;">
<tbody>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Method</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Description</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Request</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Response</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><h6 id="SafetyVideo">Safety Video<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h6></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Detects NSFW video based on parameters (porn, sexy, neutral, drawings, hentai) in input video.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Mandatory Fields: Input Video</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Gives blurred output video based on the scores for NSFW parameter. Gives byte64 video.</p></td>
</tr>
<tr>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><h6 id="NudityVideo">Nudity Video<a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h6></td>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Detects the specific parts of nudity in the given video.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Mandatory Fields: Input video</p></td>
<td class="confluenceTd" rowspan="3" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Gives the byte64 video blurring the detected nudity parts.</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p> </p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Optional Fields:  portfolio, account</p></td>
</tr>
</tbody>
</table></div>
<style>html .dt-777637637 table {min-width:0px;}</style>
</div>
</div>
</div>
<h2 id="ModelsUsed"><span data-colorid="k6rauc56g7">Models Used</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="4e3037fc-8c62-426b-a160-1643d38b16ba" data-macro-name="expand" id="expander-1669687654">
<div class="expand-control" id="expander-control-1669687654" onclick="expandContent('expander-content-1669687654', 'expander-control-1669687654')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Detoxify</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1669687654">
<h4 id="DetoxifyModel"><u>Detoxify Model</u><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>It is a machine learning model that can identify and classify toxic text. It is a powerful tool that can be used to help protect people from online abuse. Detoxify is able to identify a wide range of toxic content, including hate speech, threats, and self-harm. It can also identify more subtle forms of toxicity, such as sarcasm and insults.<br/><br/><strong>How it Works:</strong></p>
<ol start="1">
<li><p><strong>Text Preprocessing:</strong> The input text is cleaned and tokenized into a numerical representation.  </p></li>
<li><p><strong>Feature Extraction:</strong> A pre-trained language model, like BERT or RoBERTa, extracts semantic and syntactic features from the tokenized text.</p></li>
<li><p><strong>Toxicity Classification:</strong> A classifier, often a neural network, is trained to predict the likelihood of toxicity based on the extracted features. This classifier is trained on a large dataset of labeled toxic and non-toxic text.  </p></li>
<li><p><strong>Output:</strong> The model outputs a probability score for each toxicity category, such as hate speech, threats, or offensive language.<br/></p></li>
</ol>
</div>
</div>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="73e1afbb-746f-42c2-b80b-1eab0635f32b" data-macro-name="expand" id="expander-1929648185">
<div class="expand-control" id="expander-control-1929648185" onclick="expandContent('expander-content-1929648185', 'expander-control-1929648185')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">NSFW Gantman</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1929648185">
<h4 id="NSFWGantman"><u>NSFW Gantman</u><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>This AI-powered tool acts as a digital guardian, scanning images and videos to identify and flag NSFW (Not Safe For Work) material.</p>
<p>NSFW Gantman is a machine learning model designed to identify sexually explicit content in images. It works by:</p>
<ol start="1">
<li><p><strong>Image Analysis:</strong> The model analyzes the image pixel by pixel, looking for specific patterns and features associated with explicit content.</p></li>
<li><p><strong>Feature Extraction:</strong> It extracts important features from the image, such as edges, textures, and color patterns.</p></li>
<li><p><strong>Classification:</strong> Using these extracted features, the model classifies the image as either safe or explicit.</p></li>
</ol>
<p>The model is trained on a large dataset of images, allowing it to learn to recognize explicit content with high accuracy. It is widely used in content moderation systems to filter out harmful content and protect users.</p>
</div>
</div>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="9706240c-3e0d-438d-943c-5e6c39c2170b" data-macro-name="expand" id="expander-1841995335">
<div class="expand-control" id="expander-control-1841995335" onclick="expandContent('expander-content-1841995335', 'expander-control-1841995335')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Nudenet</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1841995335">
<h4 id="NudeNetModel"><u>NudeNet Model</u><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>It aims to provide a responsible AI solution for identifying and censoring nudity in various applications, which is particularly important in contexts where content moderation is necessary, such as social media platforms, adult content filtering, and user-generated content sites.</p>
<p><strong><u>Classes Detected:</u></strong></p>
<p>NudeNet can identify a variety of classes related to nudity, including:</p>
<ul>
<li><p>FEMALE_GENITALIA_EXPOSED</p></li>
<li><p>MALE_BREAST_EXPOSED</p></li>
<li><p>BUTTOCKS_EXPOSED</p></li>
<li><p>FACE_FEMALE</p></li>
<li><p>and many more.</p></li>
</ul>
</div>
</div>
<h2 id="SafetyByExamples"><span data-colorid="a5t3qo25k2">Safety - By Examples</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="0606043a-5b12-436a-a494-259e13548f16" data-macro-name="expand" id="expander-1807209988">
<div class="expand-control" id="expander-control-1807209988" onclick="expandContent('expander-content-1807209988', 'expander-control-1807209988')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Text</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1807209988">
<h4 id="UnstructuredTextSafety"><u>Unstructured Text Safety</u><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="1207" style="max-width: 760px;" width="439"><img alt="image-20241114-091013.png" class="confluence-embedded-image image-center cursor-pointer" src="github-pages/images/unknown-attachment" title="image-20241114-091013.png"/></span>
<ul>
<li><p>The AI model processes the input text and generates a safety analysis report. This report includes:</p>
<ul>
<li><p><strong>Profanity Scores:</strong></p>
<ul>
<li><p><strong>Toxicity:</strong> This metric measures the overall toxicity of the text, indicating the likelihood of it being offensive or harmful. In this case, the toxicity score is 0.973, suggesting high toxicity.</p></li>
<li><p><strong>Severe Toxicity:</strong> This metric measures the severity of the toxic content, indicating the potential for extreme harm. The score of 0.014 suggests a low level of severe toxicity.</p></li>
<li><p><strong>Obscene:</strong> This metric measures the obscenity of the content, indicating the presence of explicit or vulgar language. The score of 0.945 suggests high obscenity.</p></li>
<li><p><strong>Threat:</strong> This metric measures the threat level of the content, indicating the potential for violence or harm. The score of 0.001 suggests a very low threat level.</p></li>
</ul></li>
<li><p><strong>Profane Words:</strong></p>
<ul>
<li><p>The model identifies specific profane words present in the text: "bullshit" and "shit."</p></li>
</ul></li>
</ul></li>
</ul>
<ol start="3">
<li><p><strong>Safety Output:</strong></p></li>
</ol>
<ul>
<li><p>Based on the analysis, the AI model flags the input text as unsafe due to its high toxicity and obscenity levels.</p></li>
</ul>
<p> </p>
</div>
</div>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="ca8d59e6-ff7c-44e7-bab9-2bc99579fff0" data-macro-name="expand" id="expander-383097586">
<div class="expand-control" id="expander-control-383097586" onclick="expandContent('expander-content-383097586', 'expander-control-383097586')">
<span class="expand-control-icon icon"> </span><span class="expand-control-text">Image</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-383097586">
<h4 id="ImageSafety"><u>Image Safety</u><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p> The below provided image showcases the capabilities of an AI-powered image analysis tool. It demonstrates how the tool can effectively identify and process explicit content within an image.</p>
<ul>
<li><p><strong>Original Image:</strong> The original image contains explicit content.</p></li>
<li><p><strong>Processed Image:</strong> The tool has successfully identified and obscured the explicit parts of the image, rendering it safe for public viewing.</p></li>
</ul>
<p><strong>Pie Chart Analysis:</strong></p>
<p>The pie chart provides a detailed breakdown of the image's content categories as determined by the AI model:</p>
<ul>
<li><p><strong>Drawings:</strong> A small portion of the image consists of drawings, which are generally considered safe content.</p></li>
<li><p><strong>Hentai:</strong> A larger portion of the image is classified as hentai, a genre of anime and manga that often features explicit sexual content.</p></li>
<li><p><strong>Neutral:</strong> A relatively small portion is categorized as neutral, indicating content that is neither explicit nor suggestive.</p></li>
<li><p><strong>Porn:</strong> A significant portion of the image is classified as pornographic, meaning it contains explicit sexual content.</p></li>
<li><p><strong>Sexy:</strong> A small portion is categorized as sexy, suggesting suggestive but not explicitly sexual content.</p></li>
</ul><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="719" style="max-width: 760px;" width="173"><img alt="image (13).png" class="confluence-embedded-image image-center cursor-pointer" data-height="514" data-linked-resource-container-id="1151303933" data-linked-resource-container-version="4" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image (13).png" data-linked-resource-id="1151762622" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="5e8517fa-a784-40b6-be6e-4b26861645e2" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232359155/1151303933/media/Z3FOS3NvbzVFcW9xUU16b25IMzNzR29YWnJPMk9ERHUwdlJzMy1KOFhvUT0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRMk1qWTVPQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5NVEUxTVRNd016a3pNdz09LmFXMWhaMlVsTWpBb01UTXBMbkJ1Wnc9PS5kbVZ5YzJsdmJqMHhKbTF2WkdsbWFXTmhkR2x2YmtSaGRHVTlNVGN6TVRVM05EYzFNVGt5T1NaallXTm9aVlpsY25OcGIyNDlNU1poY0drOWRqSW1kMmxrZEdnOU1UY3pKbWhsYVdkb2REMHhNak09/image__13_.png" data-unresolved-comment-count="0" data-width="719" loading="lazy" name="image-attachment" src="github-pages/images/image__13_.png" style="width: 173px;" width="173"/><span class="image-caption"><p>Processed Image</p></span></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="502" style="max-width: 760px;" width="169"><img alt="image (12)-20241114-085626.png" class="confluence-embedded-image image-center cursor-pointer" data-height="383" data-linked-resource-container-id="1151303933" data-linked-resource-container-version="4" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image (12)-20241114-085626.png" data-linked-resource-id="1152679951" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="d6a2f545-58cf-46db-952a-57fb6aaefa3a" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232359155/1151303933/media/RjNjUWRMcElzaG5IUmRIdTh2UUZtS1pBNkotS0JzeF82WlR6MUxXNUc4ND0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRMk1qWTVPUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5NVEUxTVRNd016a3pNdz09LmFXMWhaMlVsTWpBb01USXBMVEl3TWpReE1URTBMVEE0TlRZeU5pNXdibWM9LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3pNVFUzTkRZeE5EZzNOU1pqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TVRZNUptaGxhV2RvZEQweE1qaz0=/image__12_-20241114-085626.png" data-unresolved-comment-count="0" data-width="502" loading="lazy" name="image-attachment" src="github-pages/images/image__12_-20241114-085626.png" style="width: 169px;" width="169"/><span class="image-caption"><p>Image Analyze Report</p></span></span>
<p> </p>
</div>
</div>
<p> </p>
</div>
<!-- ATTACHMENTS -->

<script>
            document.addEventListener("DOMContentLoaded", () => {
                const wrapper = document.getElementById("attachments-wrapper");
                const button = document.getElementById("toggle-attachments-view-button");
                document.querySelectorAll(".file-full").forEach(el => {
                    el.addEventListener("mouseover", moveTooltip);
                });

                button.addEventListener("click", () => {
                    wrapper.classList.toggle("attachments-wrapper-gallery");
                    wrapper.classList.toggle("attachments-wrapper-list");
                });
            });

            function moveTooltip(e) {
                if (e.target.classList.contains("file-wrapper")) {
                    let docWidth = document.body.clientWidth;
                    let docHeight = document.body.clientHeight;
                    let rect = e.target.getBoundingClientRect();
                    let fileTooltip = e.target.parentElement.querySelector(".file-tooltip")
                    if (fileTooltip) {
                        if (rect.left <= docWidth / 2) {
                            fileTooltip.classList.add("left");
                            fileTooltip.classList.remove("right");
                        } else {
                            fileTooltip.classList.remove("left");
                            fileTooltip.classList.add("right");
                        }
                        if (rect.top <= docHeight / 2) {
                            fileTooltip.classList.add("top");
                            fileTooltip.classList.remove("bottom");
                        } else {
                            fileTooltip.classList.remove("top");
                            fileTooltip.classList.add("bottom");
                        }
                    }
                }
            }

        </script>
<script>
                hideGroup('attachments');
            </script>
<div id="footer-comments-outlet">
<div>
<div class="page-comment-wrapper" data-testid="page-comment-wrapper">
<div class="cc-q82yp6">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1e0c11p5 _yv0ehpgh _727q19bv _bfhk1j28 _1bsbdgin _18u0u2gc">
<div class="_nd5lzmlf _bfhklbf8 _y3gn1h6o _1yt45uws _19itglyw _2rko1l7b"></div>
<div class="_nd5lbahz _bfhklbf8 _y3gn1h6o _1yt41h4g _19itglyw _2rko1l7b"></div>
</div>
</div>
<div class="_1sb2f705 _1e0c1ule _otyrpxbi _ca0qutpp _n7zl1l7n _1bsb1osq"></div>
<div class="_1e0c1txw _1n261g80" data-testid="comment-container">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1bsb1osq _19itglyw _2rko1l7b _4t3i1ylp _syaz9s69 _ca0qze3t _1e0c1o8l _s7n4jp4b _18u0u2gc _16jlkb7n _bfhklbf8"></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="inline-comments-outlet"></div>
</div>
</div></body><br/><br/></html>