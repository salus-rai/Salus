<!DOCTYPE html>
<html><head></head><body><div class="main-panel wiki-content">
<div class="description">
<div class="header-wrapper" style="max-width: initial">
<!-- breadcrumbs could be implemented and inserted here -->
<div></div>

</div>
<div class="header-wrapper">
<div class="title-wrapper">
<div class="title">
<h1 style="display: flex; align-items: center;">
                                        AI Content Detectors
                                    </h1>
</div>
<div class="row" style="display: flex;padding: 0; ">

</div>
</div>
</div>
<div class="content-wrapper">
<style>@media (prefers-color-scheme: dark) { }</style>
<p><style>[data-colorid=htmahwo3jj]{color:#0747a6} html[data-color-mode=dark] [data-colorid=htmahwo3jj]{color:#5999f8}[data-colorid=l5uor2q7xe]{color:#0747a6} html[data-color-mode=dark] [data-colorid=l5uor2q7xe]{color:#5999f8}[data-colorid=d9b8ye5xqk]{color:#0747a6} html[data-color-mode=dark] [data-colorid=d9b8ye5xqk]{color:#5999f8}[data-colorid=gmvxvi1jfb]{color:#0747a6} html[data-color-mode=dark] [data-colorid=gmvxvi1jfb]{color:#5999f8}[data-colorid=grol2w46xc]{color:#0747a6} html[data-color-mode=dark] [data-colorid=grol2w46xc]{color:#5999f8}[data-colorid=g2kb41h2u2]{color:#0747a6} html[data-color-mode=dark] [data-colorid=g2kb41h2u2]{color:#5999f8}[data-colorid=jpanvjc9qn]{color:#0747a6} html[data-color-mode=dark] [data-colorid=jpanvjc9qn]{color:#5999f8}[data-colorid=x2ey2h7vqs]{color:#0747a6} html[data-color-mode=dark] [data-colorid=x2ey2h7vqs]{color:#5999f8}[data-colorid=lho8p6fbg9]{color:#0747a6} html[data-color-mode=dark] [data-colorid=lho8p6fbg9]{color:#5999f8}</style></p>
<div class="toc-macro client-side-toc-macro conf-macro output-block" data-cssliststyle="disc" data-hasbody="false" data-headerelements="H1,H2,H3,H4,H5,H6" data-layout="default" data-local-id="0505ad81-4d26-4932-b1f5-b77f2d11ab38" data-macro-id="706ff877-2dc7-4a47-ba20-88e323b7dec8" data-macro-name="toc" data-numberedoutline="false" data-structure="list">
<ul>
<li><a class="not-blank" href="#DetectionofLLMGeneratedTextBinoculars"><span data-colorid="l5uor2q7xe">Detection of LLM Generated Text - Binoculars</span></a></li>
<li><a class="not-blank" href="#TestResults"><span data-colorid="d9b8ye5xqk">Test Results</span></a></li>
<li><a class="not-blank" href="#Links"> <span data-colorid="jpanvjc9qn">Links</span></a></li>
<li><a class="not-blank" href="#SynthIDText"><span data-colorid="g2kb41h2u2">SynthID-Text</span></a></li>
</ul>
</div>
<h2 id="DetectionofLLMGeneratedTextBinoculars"><span data-colorid="l5uor2q7xe">Detection of LLM Generated Text - Binoculars</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p>Binoculars propose measuring whether the tokens that appear in a string are surprising relative to the baseline perplexity of an LLM acting on the same string. A string might have properties that result in high perplexity when completed by any agent, machine or human. But the expectation is that next-token choices of humans to be even higher perplexity than those of a machine. By normalizing the observed perplexity by the expected perplexity of a machine acting on the same text, we can arrive at a detection metric that is fairly invariant to the prompt. The Binoculars score is a general mechanism that captures a statistical signature of machine text.</p>
<p>Formula:</p>
<ol start="1">
<li><p>Probability of next token</p></li>
</ol>
<p>A string of characters s can be parsed into tokens and represented as a list of token indices ⃗x by a tokenizer T. Let xi denote the token ID of the i-th token, which refers to an entry in the LLMs vocabulary V = {1, 2..., n}. Given a token sequence as input, a language model M predicts the next token by outputting a probability distribution over the vocabulary:</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="296" style="max-width: 760px;" width="296"><img alt="image-20240417-010332.png" class="confluence-embedded-image image-center cursor-pointer" data-height="59" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240417-010332.png" data-linked-resource-id="742523473" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="8580fa02-bd14-4869-bcfd-d2ee6210efc9" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/ekZNTlFYZDkxSlNXTWNyYkQxU2RXQndVNTdEbm9KbWtaeHFqVXBveUt6UT0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuYVcxaFoyVXRNakF5TkRBME1UY3RNREV3TXpNeUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3hOVGcxTnpReU5qa3pOeVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TWprMkptaGxhV2RvZEQwMU9BPT0=/image-20240417-010332.png" data-unresolved-comment-count="0" data-width="296" loading="lazy" name="image-attachment" src="github-pages/images/image-20240417-010332.png" style="width: 296px" width="296"/></span>
<ol start="2">
<li><p>Perplexity - Average negative log-likelihood of all tokens</p></li>
</ol>
<p>We will abuse notation and abbreviate M(T(s)) as M(s) where the tokenizer is implicitly the one used in training M. For our purposes, we define log PPL, the log-perplexity, as the average negative log-likelihood of all tokens in the given sequence. Formally, let</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="537" style="max-width: 760px;" width="537"><img alt="image-20240417-012403.png" class="confluence-embedded-image image-center cursor-pointer" data-height="90" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240417-012403.png" data-linked-resource-id="742523479" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="da0ac069-5251-4cf8-ae04-3f2feeae5ee1" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/VWhveWI0U1V6TFEtRmF2N2NuNFFkX2Y1WkZKQ1ZlQVV3d2MyX0cwV1g2QT0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuYVcxaFoyVXRNakF5TkRBME1UY3RNREV5TkRBekxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3hOVGcxTnpReU5qazFOeVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TlRNM0ptaGxhV2RvZEQwNU1BPT0=/image-20240417-012403.png" data-unresolved-comment-count="0" data-width="537" loading="lazy" name="image-attachment" src="github-pages/images/image-20240417-012403.png" style="width: 537px" width="537"/></span>
<ol start="3">
<li><p>Cross Perplexity - How surprising the output of one model to another</p></li>
</ol>
<p>Intuitively, log-perplexity measures how “surprising” a string is to a language model. As mentioned above, perplexity has been used to detect LLMs, as humans produce more surprising text than LLMs. This is reasonable, as log PPL is also the loss function used to train generative LLMs, and models are likely to score their own outputs as unsurprising.</p>
<p>Our method also measures how surprising the output of one model is to another. We define the cross-perplexity, which takes two models and a string as its arguments. Let log X-PPLM1,M2 (s) measure the average per-token cross-entropy between the outputs of two models, M1 and M2 , when operating on the tokenization of s.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="509" style="max-width: 760px;" width="509"><img alt="image-20240417-010625.png" class="confluence-embedded-image image-center cursor-pointer" data-height="78" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240417-010625.png" data-linked-resource-id="742523485" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="db35ac16-10b3-4894-bdb9-02b10c53f674" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/Y0VSVzk1QUJrbjVTN0h1NVlnNEdkMWZ3RVB1VVRmekUzSDV3d3htb0lSVT0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuYVcxaFoyVXRNakF5TkRBME1UY3RNREV3TmpJMUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3hOVGcxTnpReU5qazNOeVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TlRBNUptaGxhV2RvZEQwM09BPT0=/image-20240417-010625.png" data-unresolved-comment-count="0" data-width="509" loading="lazy" name="image-attachment" src="github-pages/images/image-20240417-010625.png" style="width: 509px" width="509"/></span>
<p>Note that · denotes the dot product between two vector-valued quantities</p>
<ol start="4">
<li><p>Binoculars Score</p></li>
</ol>
<p>Binoculars score B as a sort of normalization or reorientation of perplexity. In particular we look at the ratio of perplexity to cross-perplexity.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="343" style="max-width: 760px;" width="343"><img alt="image-20240417-011206.png" class="confluence-embedded-image image-center cursor-pointer" data-height="72" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240417-011206.png" data-linked-resource-id="742523491" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="a3d82aba-3784-4738-8275-3217594746f8" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/Ri1hYnl4WThieTdnVlk5T0l2eTBfUkZPOE9EUTZiT0wyQkZqTGIwRnByYz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuYVcxaFoyVXRNakF5TkRBME1UY3RNREV4TWpBMkxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3hOVGcxTnpReU5qazVOaVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TXpRekptaGxhV2RvZEQwM01nPT0=/image-20240417-011206.png" data-unresolved-comment-count="0" data-width="343" loading="lazy" name="image-attachment" src="github-pages/images/image-20240417-011206.png" style="width: 343px" width="343"/></span>
<p>Here, the numerator is simply the perplexity, which measures how surprising a string is to M1. The denominator measures how surprising the token predictions of M2 are when observed by M1. Intuitively, we expect a human to diverge from M1 more than M2 diverges from M1, provided the LLMs M1 and M2 are more similar to each other than they are to a human. The Binoculars score is a general mechanism that captures a statistical signature of machine text. In the sections below, we show that for most obvious choices of M1 and M2, Binoculars does separate machine and human text much better than perplexity alone. Importantly, it is capable of detecting generic machine-text generated by a third model altogether</p>
<p> </p>
<p>Different combinations of scoring models, evaluated on datasets: </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="686" style="max-width: 760px;" width="686"><img alt="Screenshot (21)-20240517-092325.png" class="confluence-embedded-image image-center cursor-pointer" data-height="379" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="Screenshot (21)-20240517-092325.png" data-linked-resource-id="747012168" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="69422aef-a54a-4148-9236-fdd1eb3b7ca2" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/eTdVdm9SZnluTFZpOUxSWHFIaHpTVUU0dXAzZHFBZHlSUU9MOUFUNW54Yz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuVTJOeVpXVnVjMmh2ZENVeU1DZ3lNU2t0TWpBeU5EQTFNVGN0TURreU16STFMbkJ1Wnc9PS5kbVZ5YzJsdmJqMHhKbTF2WkdsbWFXTmhkR2x2YmtSaGRHVTlNVGN4TlRrek56Z3hPRGM0TlNaallXTm9aVlpsY25OcGIyNDlNU1poY0drOWRqSW1kMmxrZEdnOU5qZzJKbWhsYVdkb2REMHpOems9/Screenshot__21_-20240517-092325.png" data-unresolved-comment-count="0" data-width="686" loading="lazy" name="image-attachment" src="github-pages/images/Screenshot__21_-20240517-092325.png" style="width: 686px" width="686"/></span>
<p>As shown in table, it is observed that using Falcon-7B as input models has some competitive edge in performance. So, we are using Falcon-7B and Falcon-7B-instruct models as X-Cross PPL Scorer and PPL Scorer.</p>
<h2 id="TestResults"><span data-colorid="d9b8ye5xqk">Test Results</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p>We have tested the accuracy with three different data sets.</p>
<ol start="1">
<li><p>500 texts generated by human</p></li>
<li><p>500 texts generated by AI</p></li>
<li><p>50 texts were generated by AI with the prompt (make it like written by human)</p></li>
</ol><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="495" style="max-width: 760px;" width="495"><img alt="image-20240417-013015.png" class="confluence-embedded-image image-center cursor-pointer" data-height="355" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240417-013015.png" data-linked-resource-id="742523497" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="4d55c96b-a2df-4f06-8af3-8f7438e94417" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/czBoalBWYTFTS1pVeXd1ZWtFZi1KYkpma25ORm5sMEh6QlFPZ1VtREplYz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuYVcxaFoyVXRNakF5TkRBME1UY3RNREV6TURFMUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3hOVGcxTnpReU56QXhOaVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TkRrMUptaGxhV2RvZEQwek5UUT0=/image-20240417-013015.png" data-unresolved-comment-count="0" data-width="495" loading="lazy" name="image-attachment" src="github-pages/images/image-20240417-013015.png" style="width: 495px" width="495"/></span>
<h2 id="Links"> <span data-colorid="jpanvjc9qn">Links</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p>Paper - <a class="external-link" href="https://arxiv.org/abs/2401.12070" rel="nofollow" target="_blank">https://arxiv.org/abs/2401.12070</a> </p>
<h2 id="SynthIDText"><span data-colorid="g2kb41h2u2">SynthID-Text</span><a aria-label-top="Copy link to heading" class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p>Link of the research paper: <a class="external-link" href="https://www.nature.com/articles/s41586-024-08025-4" rel="nofollow" target="_blank">Scalable watermarking for identifying large language model outputs | Nature</a></p>
<p><span data-colorid="gmvxvi1jfb">Introduction</span></p>
<p>In today's rapidly evolving technological landscape, artificial intelligence (AI) has significantly transformed various sectors, including content creation. While the potential benefits of AI are substantial, challenges arise, especially in distinguishing between content created by humans and that generated by AI. To tackle this issue, Google DeepMind has developed SynthID, an innovative technology aimed at identifying AI-generated text.</p>
<p>One effective strategy for addressing the challenge of distinguishing between human and AI-generated content is <strong>text watermarking</strong>. This technique embeds a subtle signature within the generated text, allowing for its later identification. A notable approach within this realm is <strong>generative watermarking</strong>, which integrates the watermark directly into the text generation process. By modifying the probability distribution of the next token, the language model (LLM) introduces a statistical signature that can be detected later. This method has the advantage of minimal computational overhead and does not require access to the underlying LLM, making it suitable for large-scale implementation.</p>
<p><span data-colorid="x2ey2h7vqs">How Generative Watermarking Works</span></p>
<p>The text generation process of an LLM is inherently autoregressive. It assigns probabilities to various vocabulary elements and selects the next token based on these probabilities. Generative watermarking exploits this autoregressive nature by altering the next-token sampling procedure. This adjustment introduces unique context-specific alterations to the generated text, resulting in a distinct statistical signature. During detection, this signature can be analyzed to ascertain if the text originated from the watermarked LLM.</p>
<p> </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="464" style="max-width: 760px;" width="623"><img alt="image-20241112-123813.png" class="confluence-embedded-image image-center cursor-pointer" data-height="301" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20241112-123813.png" data-linked-resource-id="1142654060" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="c3063d80-5d7e-4c3c-94bc-5be4a769c4ac" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/dWxIdkVDOXctM0pZbGUwaEVmMks5QmlNZHJ6emt3TEk3QmtuUm9aY0t5RT0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuYVcxaFoyVXRNakF5TkRFeE1USXRNVEl6T0RFekxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3pNVFF4TlRBNU5qZzVNeVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TmpJekptaGxhV2RvZEQwME1ETT0=/image-20241112-123813.png" data-unresolved-comment-count="0" data-width="464" loading="lazy" name="image-attachment" src="github-pages/images/image-20241112-123813.png" style="width: 623px;" width="623"/></span>
<p> </p>
<p><strong>SynthID-Text</strong> represents a novel generative watermarking scheme that enhances existing techniques. It introduces a new sampling algorithm known as <strong>Tournament sampling</strong>, which can be configured for either non-distortionary (preserving text quality) or distortionary (enhancing watermark detectability) modes. In both configurations, SynthID-Text demonstrates superior detection rates compared to previous methods. The non-distortionary mode has already been successfully integrated into systems like Gemini and Gemini Advanced, showcasing its practical applicability in real-world scenarios.</p>
<p><span data-colorid="htmahwo3jj">Watermarking with SynthID-Text</span></p>
<p>LLMs generate text by considering preceding context, such as responding to a prompt. Given a sequence of tokens, the LLM calculates the probability distribution of the next token based on prior context. The generative watermarking scheme comprises three main components: a random seed generator, a sampling algorithm, and a scoring function. The random seed generator provides a seed for each generation step, while the sampling algorithm uses this seed to select the next token, introducing correlations that are measurable during watermark detection.</p>
<p>The <strong>Tournament sampling</strong> method is particularly noteworthy. It employs a tournament-like process to select an output token based on scores from multiple watermarking functions. By sampling multiple candidate tokens and iteratively eliminating lower-scoring options, the algorithm identifies a final output token that embeds the watermark.</p>
<p> </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="404" style="max-width: 760px;" width="596"><img alt="image-20241112-123946.png" class="confluence-embedded-image image-center cursor-pointer" data-height="374" data-linked-resource-container-id="743866377" data-linked-resource-container-version="9" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20241112-123946.png" data-linked-resource-id="1143046880" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="b50335c5-f917-4ead-9941-3c34a5df9be2" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/232523122/743866377/media/STRSVzhzRFhoMWJUTDFBdVJsOTJIYTcwTWhOMldBcUN1Q01oaGNxRm9Zcz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVGswTmpRNU9UVTRNUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5OelF6T0RZMk16YzMuYVcxaFoyVXRNakF5TkRFeE1USXRNVEl6T1RRMkxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3pNVFF4TlRFNU1EQXhNeVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TlRrMkptaGxhV2RvZEQwMU5UST0=/image-20241112-123946.png" data-unresolved-comment-count="0" data-width="404" loading="lazy" name="image-attachment" src="github-pages/images/image-20241112-123946.png" style="width: 596px;" width="596"/></span>
<p> </p>
<p><span data-colorid="lho8p6fbg9">Watermark Detection</span></p>
<p>To detect watermarked text, SynthID-Text calculates a score based on the alignment between the generated text and the random seeds used during its creation. A higher score indicates a greater likelihood that the text is watermarked. Factors such as text length and the entropy of the LLM distribution significantly influence detection performance. Longer texts provide more evidence, while higher entropy allows for a broader range of token selections, thereby enhancing watermark strength.</p>
<p><span data-colorid="grol2w46xc">Limitations</span></p>
<ol start="1">
<li><p>Model Dependency: SynthID can only identify AI-generated content if it was created using a model that incorporates SynthID's watermarking technology. If a model doesn't utilize SynthID, the technology will be ineffective. Currently only the models that are developed by Google have this SynthID watermarking integrated.</p></li>
</ol>
<ol start="2">
<li><p>Watermark application is less effective on factual responses, as there is less opportunity to augment generation without decreasing accuracy.</p></li>
</ol>
<ol start="3">
<li><p>Detector confidence scores can be greatly reduced when an AI-generated text is thoroughly rewritten or translated to another language.</p></li>
</ol>
<p>In summary, while SynthID and generative watermarking represent significant advancements in the identification of AI-generated text, challenges remain in ensuring comprehensive applicability and effectiveness across diverse contexts. As AI continues to evolve, so too will the methods for distinguishing its outputs from human-created content.</p>
<p> </p>
<p> </p>
<p> </p>
</div>
<!-- ATTACHMENTS -->

<script>
            document.addEventListener("DOMContentLoaded", () => {
                const wrapper = document.getElementById("attachments-wrapper");
                const button = document.getElementById("toggle-attachments-view-button");
                document.querySelectorAll(".file-full").forEach(el => {
                    el.addEventListener("mouseover", moveTooltip);
                });

                button.addEventListener("click", () => {
                    wrapper.classList.toggle("attachments-wrapper-gallery");
                    wrapper.classList.toggle("attachments-wrapper-list");
                });
            });

            function moveTooltip(e) {
                if (e.target.classList.contains("file-wrapper")) {
                    let docWidth = document.body.clientWidth;
                    let docHeight = document.body.clientHeight;
                    let rect = e.target.getBoundingClientRect();
                    let fileTooltip = e.target.parentElement.querySelector(".file-tooltip")
                    if (fileTooltip) {
                        if (rect.left <= docWidth / 2) {
                            fileTooltip.classList.add("left");
                            fileTooltip.classList.remove("right");
                        } else {
                            fileTooltip.classList.remove("left");
                            fileTooltip.classList.add("right");
                        }
                        if (rect.top <= docHeight / 2) {
                            fileTooltip.classList.add("top");
                            fileTooltip.classList.remove("bottom");
                        } else {
                            fileTooltip.classList.remove("top");
                            fileTooltip.classList.add("bottom");
                        }
                    }
                }
            }

        </script>
<script>
                hideGroup('attachments');
            </script>
<div id="footer-comments-outlet">
<div>
<div class="page-comment-wrapper" data-testid="page-comment-wrapper">
<div class="cc-q82yp6">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1e0c11p5 _yv0ehpgh _727q19bv _bfhk1j28 _1bsbdgin _18u0u2gc">
<div class="_nd5lzmlf _bfhklbf8 _y3gn1h6o _1yt45uws _19itglyw _2rko1l7b"></div>
<div class="_nd5lbahz _bfhklbf8 _y3gn1h6o _1yt41h4g _19itglyw _2rko1l7b"></div>
</div>
</div>
<div class="_1sb2f705 _1e0c1ule _otyrpxbi _ca0qutpp _n7zl1l7n _1bsb1osq"></div>
<div class="_1e0c1txw _1n261g80" data-testid="comment-container">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">
<div class="_bfhklbf8 _1bsbzwfg _4t3izwfg _2rko1ssb _19pk1b66 _2hwxutpp"></div>
<div class="_1bsb1osq _19itglyw _2rko1l7b _4t3i1ylp _syaz9s69 _ca0qze3t _1e0c1o8l _s7n4jp4b _18u0u2gc _16jlkb7n _bfhklbf8"></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="inline-comments-outlet"></div>
</div>
</div></body><br/><br/></html>